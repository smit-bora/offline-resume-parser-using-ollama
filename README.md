```markdown
# ğŸ“„ Offline Resume Parser & Candidate Ranking System

Parse PDF resumes using **offline AI models (Ollama)** and rank candidates based on a given job description â€” **no cloud, no data leakage**.

---

## ğŸ§  What This Project Does

- ğŸ“‚ Upload PDF resumes  
- ğŸ¤– Uses an **offline AI model** to understand resumes  
- ğŸ§¾ Extracts structured data:
  - Skills
  - Education
  - Experience
- ğŸ“Š Ranks candidates based on job description relevance
- ğŸŒ Clean and simple web interface

---

## ğŸ–¥ï¸ System Requirements

- Windows / macOS / Linux
- **Python 3.8 or higher**
- **8 GB RAM recommended**
- Internet required **only once** to download dependencies and AI model

---

## ğŸ§© Tech Stack Used

- **Python** â€“ backend logic
- **FastAPI** â€“ backend API framework
- **Ollama** â€“ runs AI models locally (offline)
- **Llama 3.2** â€“ resume parsing & ranking
- **HTML / CSS / JavaScript** â€“ frontend UI

---

---

## ğŸ› ï¸ Step-by-Step Setup

---

### 1ï¸âƒ£ Install Python

1. Download Python from:  
   ğŸ‘‰ https://www.python.org/downloads/
2. During installation:
   - âœ… **Check** â€œAdd Python to PATHâ€
3. Verify installation:
   ```bash
   python --version
````

You should see something like:

```
Python 3.10.x
```

---

### 2ï¸âƒ£ Install Ollama (Offline AI Engine)

1. Download Ollama from:
   ğŸ‘‰ [https://ollama.com](https://ollama.com)
2. Install it normally
3. Verify installation:

   ```bash
   ollama --version
   ```

---

### 3ï¸âƒ£ Download or Clone the Project

Clone the repository:

```bash
git clone <your-repository-url>
```

OR download ZIP and extract it.

Navigate to the backend folder:

```bash
cd backend
```

---

### 4ï¸âƒ£ Create a Virtual Environment (IMPORTANT)

This prevents dependency conflicts.

```bash
python -m venv venv
```

Activate it:

**Windows**

```bash
venv\Scripts\activate
```

**macOS / Linux**

```bash
source venv/bin/activate
```

You should now see `(venv)` in the terminal.

---

### 5ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

### 6ï¸âƒ£ Download the AI Model (One-Time Only)

```bash
ollama pull llama3.2:latest
```

This downloads the model locally so it can run **offline**.

---

### 7ï¸âƒ£ Create `.env` Configuration File

Inside the `backend/` folder, create a file named `.env` and paste **exactly this**:

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
MAX_FILE_SIZE=10485760
DEBUG=True
PORT=8000

RUN_ANALYSIS=False
ANALYSIS_MODEL=llama3.2:latest
ANALYSIS_TEMPERATURE=0.1
```

---

### 8ï¸âƒ£ Create Required Folder & Files

#### Create uploads folder:

```bash
mkdir uploads
```

#### Ensure every folder inside `backend/app/` has an empty `__init__.py` file

Example:

```
backend/app/api/__init__.py
backend/app/core/__init__.py
backend/app/model/__init__.py
```

---

### 9ï¸âƒ£ Start the Backend Server

Make sure:

* `(venv)` is active
* You are inside `backend/`

Run:

```bash
uvicorn app.main:app --reload
```

If successful, you will see:

```
Uvicorn running on http://127.0.0.1:8000
```

âš ï¸ **Do NOT close this terminal while using the app**

---

## ğŸŒ Running the Frontend

1. Open this file in your browser:

```
frontend/index.html
```

2. Inside the app:

   1. Upload a PDF resume
   2. Wait for parsing to finish
   3. Paste job description
   4. View ranked candidates

---

## â— Common Errors & Fixes

### âŒ `ollama not found`

* Restart system after installing Ollama
* Ensure Ollama is added to PATH

---

### âŒ `ModuleNotFoundError`

* Virtual environment not activated

```bash
venv\Scripts\activate
```

---

### âŒ Frontend not working

* Backend server must be running
* Check terminal logs for errors

---

## ğŸ” Privacy & Offline Guarantee

* âœ… No cloud APIs used
* âœ… Resumes never leave your system
* âœ… AI runs fully offline

---

## ğŸš€ Who Is This Project For?

* Students learning AI / LLMs
* Recruiters needing offline resume screening
* HR-tech applications
* Privacy-focused AI projects


